# install.packages('xtable')
library(xtable)
library(PoEdata)

#install.packages('knitr')
library(knitr) # helps to print pretty tables


library(tseries)      
#install.packages('tseries') # JBtest for normality 
library(tseries)


rm(list=ls()) # Caution: this clears the Environment



# 4.1 
# Find the prediction 
data("food")

# familiarize with "food"
plot(food$income, food$food_exp)
mod = lm(food_exp~income, data=food)
summary(mod)
vcov(mod)



# 

alpha <- 0.05
x <- 20
xbar <- mean(food$income)
m1 <- lm(food_exp~income, data=food)
b1 <- coef(m1)[[1]]
b2 <- coef(m1)[[2]]
yhatx <- b1+b2*x
yhatx
sm1 <- summary(m1)
df <- df.residual(m1)
tcr <- qt(1-alpha/2, df)
N <- nobs(m1)   #number of observations, N
N <- NROW(food) #just another way of finding N
varb2 <- vcov(m1)[2, 2]
sighat2 <- sm1$sigma^2 # estimated variance
varf <- sighat2+sighat2/N+(x-xbar)^2*varb2 #forecast variance
sef <- sqrt(varf) #standard error of forecast
lb <- yhatx-tcr*sef
ub <- yhatx+tcr*sef

c(lb, ub)

# The result is the prediction interval for the forecast  (104.13,471.09)
# Let us calculate "prediction intervals" of the forecast for all the observations in the sample and draw the upper and lower limits together with the regression line
sef <- sqrt(sighat2+sighat2/N+(food$income-xbar)^2*varb2)
yhatv <- fitted.values(m1)
lbv <- yhatv-tcr*sef
ubv <- yhatv+tcr*sef
xincome <- food$income
dplot <- data.frame(xincome, yhatv, lbv, ubv)  
dplotord <- dplot[order(xincome), ]
xmax <- max(dplotord$xincome)
xmin <- min(dplotord$xincome)
ymax <- max(dplotord$ubv)
ymin <- min(dplotord$lbv)


# Overlay the scatter plot with the prediction and prediciton intervals 
plot(food$income, food$food_exp, 
     xlim=c(xmin, xmax), 
     ylim=c(ymin, ymax), 
     xlab="income", ylab="food expenditure")
# prediction
lines(dplotord$yhatv~ dplotord$xincome, lty=1)
# prediction intervals
lines(dplotord$ubv~dplotord$xincome, lty=2)
lines(dplotord$lbv~dplotord$xincome, lty=2)


# A different way of finding point and interval estimates for the predicted  E(y|x)E(y|x)  and forecasted  yy 
incomex=data.frame(income=20)
predict(m1, newdata=incomex, interval="confidence",level=0.95)
predict(m1, newdata=incomex, interval="prediction",level=0.95)


# We will not use the folloing wordings. 
#you may think about the distinction between the two types of intervals that we called prediction and forecast as follows: the prediction interval is not supposed to include, say, 95 percent of the points, but to include the regression line,  E(y|x)E(y|x) , with a probability of 95 percent; the forecasted interval, on the other hand, should include any true point with a 95 percent probability.
xmin <- min(food$income)
xmax <- max(food$income)
income <- seq(from=xmin, to=xmax)
ypredict <- predict(m1, newdata=data.frame(income),
                  interval="confidence")
yforecast <- predict(m1, newdata=data.frame(income),
                  interval="predict")
matplot(income, cbind(ypredict[,1], ypredict[,2], ypredict[,3], 
                      yforecast[,2], yforecast[,3]), 
        type ="l", lty=c(1, 2, 2, 3, 3), 
        col=c("black", "red", "red", "blue", "blue"),
        ylab="food expenditure", xlab="income")
points(food$income, food$food_exp)
legend("topleft", 
        legend=c("E[y|x]", "lwr_pred", "upr_pred",
                "lwr_forcst","upr_forcst"),
        lty=c(1, 2, 2, 3, 3), 
        col=c("black", "red", "red", "blue", "blue")
        )



# 4.2 Goodness-of-fit

# how to obtain R^2
(rsq <- sm1$r.squared) #or
sm1 
# If you need SSR or SSE, use ANOVA
anov <- anova(m1)
anov
dfr <- data.frame(anov)
kable(dfr, 
  caption="Output generated by the `anova` function")
SSE = anov[2,2];
SSR = anov[1,2];
SST = SSE+SSR;
R2 = SSR/ SST; 
R2



# 4.2* linear-log models: y = beta1+beta2 ln(x) + e
# Let us estimate a linear-log model for the food dataset, draw the regression curve, and calculate the marginal effects for some given values of the dependent variable.
data(food)

plot(food$income, food$food_exp)
mod2 <- lm(food_exp~log(income), data=food)
tbl <- data.frame(xtable(mod2))
kable(tbl, digits=5, 
      caption="Linear-log model output for the *food* example")
b1 <- coef(mod2)[[1]]
b2 <- coef(mod2)[[2]]
income = food$income;
pmod2 <- predict(mod2, newdata=data.frame(income),
                interval="confidence")
plot(food$income, food$food_exp, xlab="income", 
     ylab="food expenditure")
lines(pmod2[,1]~income, lty=1, col="black")
lines(pmod2[,2]~income, lty=2, col="red")
lines(pmod2[,3]~income, lty=2, col="red")

x <- 10 #for a household earning #1000 per week
y <- b1+b2*log(x)
DyDx <- b2/x    #marginal effect
DyPDx <- b2/100 #1% change in x leads to (beta2/100)-unit change in y. 
PDyPDx <- b2/y  #elasticity
DyDx
DyPDx
PDyPDx

# The results for an income of $1000 are as follows:  dy/dx=13.217dy/dx=13.217 , which indicates that an increase in income of $100 (i.e., one unit of  xx ) increases expenditure by $  13.21713.217 ; for a 1% increase in income, that is, an increase of $10, expenditure increases by $  1.3221.322 ; and, finally, for a 1% increase in income expenditure incrases by  0.6380.638 %.


# 4.3 Modeling issues
# The residuals of the of the linear-log equation of the food expenditure example. One can notice that the spread of the residuals seems to be higher at higher incomes, which may indicate that the heteroskedasticity assumption is violated.
mod2 <- lm(food_exp~log(income), data=food)
ehat <- mod2$residuals
plot(food$income, ehat, xlab="income", ylab="residuals")

# Consider a simple moded: y = 1+x+e, create artificial dataset, and check residuals
#set.seed(12345)   #sets the seed for the random number generator
x <- runif(300, 0, 10)
x
hist(x)

e <- rnorm(300, 0, 1) # error
hist(e)

y <- 1+x+e
y
plot(x,y)
mod3 <- lm(y~x)

plot(mod3) # This one helps to give multiple nice residuals plot. 
ehat <- resid(mod3)
plot(x)
plot(x,ehat, xlab="x", ylab="residuals")
hist(ehat)
qqnorm(ehat)

# Consider a different model: y = 15 - 4x^2 +e 
# set.seed(12345)
x <- runif(1000, -2.5, 2.5)
hist(x)
e <- rnorm(1000, 0, 4)
y <- 15-4*x^2+e
plot(x,y)

# see some curvature in the data

mod3 <- lm(y~x)
ehat <- resid(mod3)
ymi <- min(ehat)
yma <- max(ehat)
# step 1. 
plot(x, ehat, ylim=c(ymi, yma),
     xlab="x", ylab="residuals",col="grey")

# step 2
hist(ehat)
# step 3 
qqnorm(ehat)

# Use Jarque-Bera test for normaltiy check. While the histogram may not strongly support one conclusion or another about the normlity of ehat, the Jarque-Bera test is unambiguous: there is no evidence against the normality hypothesis.
install.packages('tseries')
library(tseries)
data(food)
plot(food$income, food$food_exp)
mod1 <- lm(food_exp~income, data=food)
ehat <- resid(mod1)
plot(ehat)
ebar <- mean(ehat)
sde <- sd(ehat)
hist(ehat)
qqnorm(ehat)
hist(ehat, col="grey", freq=FALSE, main="",
     ylab="density", xlab="ehat")
curve(dnorm(x, ebar, sde), col=2, add=TRUE,
      ylab="density", xlab="ehat")

jarque.bera.test(ehat) #(in package 'tseries')

# With JB test, because the p-value = 0.9688 > 0.05 (alpha)
# We do not reject H0. So, we conclude that the residuals
# are likely to be normally distributed. 


# 4.3 Linear-log model


data(food)
plot(food$income, food$food_exp)
hist(food$income)
hist(food$food_exp)

hist(log(food$income))
hist(log(food$food_exp))


mod2 <- lm(food_exp~log(income), data=food)
ehat <- resid(mod2)
plot(ehat)

mod2
tbl <- data.frame(xtable(mod2))
kable(tbl, digits=5, 
      caption="Linear-log model output for the *food* example")
b1 <- coef(mod2)[[1]]
b2 <- coef(mod2)[[2]]

x <- 10 #for a household earning #1000 per week
 y <- b1+b2*log(x)
 DyDx <- b2/x    #marginal effect
 DyPDx <- b2/100 #1% change in x leads to (beta2/100)-unit change in y. #mistate in the webpage
 PDyPDx <- b2/y  #elasticity
 DyDx
 DyPDx
PDyPDx
0.638061



# 4.4 polynomials

data("wa_wheat")
mod1 <- lm(greenough~time, data=wa_wheat)
ehat <- resid(mod1)
plot(wa_wheat$time, ehat, xlab="time", ylab="residuals")

# a pattern in the residuals. Consider another model.  
mod2 <- lm(wa_wheat$greenough~I(time^3), data=wa_wheat)
ehat <- resid(mod2)
plot(wa_wheat$time, ehat, xlab="time", ylab="residuals")

# 4.5. log-linear
# Example: 
# The general log-linear model: log(y) = beta1+beta2 x +e .
# Let us do the prediction: $\hat{y}_n$ and $\hat{y}_c$. 
# marginal effect, semi-elasticity. 
data(wa_wheat)
mod4 <- lm(log(greenough)~time, data=wa_wheat)
smod4 <- summary(mod4)
tbl <- data.frame(xtable(smod4))
kable(tbl, caption="Log-linear model for the *yield* equation")
# b2=0.017844b2=0.017844, this indicates that the rate of growth in wheat production has increased at an average rate of approximately  1.781.78  percent per year.
# The wage log-linear equation provides another example of calculating a growth rate, but this time the independent variable is not  timetime , but  educationeducation . The predictions and the slope are calculated for  educ=12educ=12  years.
data("cps4_small")
?cps4_small
xeduc <- 12
plot(cps4_small$edu, cps4_small$wage)

mod5 <- lm(log(wage)~educ, data=cps4_small)
summary(mod5)
tabl <- data.frame(xtable(smod5))
kable(tabl, caption="Log-linear 'wage' regression output")

b1 <- coef(smod5)[[1]]
b2 <- coef(smod5)[[2]]
sighat2 <- smod5$sigma^2
g <- 100*b2               #growth rate
yhatn <- exp(b1+b2*xeduc) #"natural" predictiction
yhatc <- exp(b1+b2*xeduc+sighat2/2) #corrected prediction
DyDx <- b2*yhatn          #marginal effect
# Here are the results of these calculations: ¡§natural¡¨ prediction 
$y_n=14.796$; corrected prediction,  $y_c=16.996$; growth rate  $g$=9.041g; and marginal effect 
$dy/dx=1.34$. The growth rate indicates that an increase in education by one unit (see the data description using ?cps4_small) increases hourly wage by  9.041  percent.
We present the ¡§natural¡¨ and the ¡§corrected¡¨ regression lines for the wage equation, together with the actual data points.

education=seq(0,22,2)
yn <- exp(b1+b2*education)
yc <- exp(b1+b2*education+sighat2/2)
plot(cps4_small$educ, cps4_small$wage, 
     xlab="education", ylab="wage", col="grey")
lines(yn~education, lty=2, col="black")
lines(yc~education, lty=1, col="blue")
legend("topleft", legend=c("yc","yn"), 
       lty=c(1,2), col=c("blue","black")

# The regular  $R^2$  cannot be used to compare two regression models having different dependent variables such as a linear-log and a log-linear models; 
# when such a comparison is needed, one can use the general  $R^2$, 
# which is  R2g=[corr(y,y^]2. Let us calculate the generalized $R^2$  for the quadratic and the log-linear wage models.
mod4 <- lm(wage~I(educ^2), data=cps4_small)
yhat4 <- predict(mod4)
mod5 <- lm(log(wage)~educ, data=cps4_small)
smod5 <- summary(mod5)
b1 <- coef(smod5)[[1]]
b2 <- coef(smod5)[[2]]
sighat2 <- smod5$sigma^2
yhat5 <- exp(b1+b2*cps4_small$educ+sighat2/2)
rg4 <- cor(cps4_small$wage, yhat4)^2
rg5 <- cor(cps4_small$wage,yhat5)^2
rg4
rg5

# The quadratic model yields  R2g=0.188, and the log-model yields  R2g=0.186; 
since the former is higher, we conclude that the quadratic model is a better fit to the data than the log-linear one. 

# Now, we find the forecast interval estimate. 
# The for the wage in the log-linear model
# Prediction interval for educ = 12
alpha <- 0.05
xeduc <- 12
xedbar <- mean(cps4_small$educ)
xedbar
mod5 <- lm(log(wage)~educ, data=cps4_small)
smod5 <-summary(mod5)
smod5
b1 <- coef(mod5)[[1]]
b2 <- coef(mod5)[[2]]
df5 <- mod5$df.residual
N <- nobs(mod5)
tcr <- qt(1-alpha/2, df=df5)
smod5 <- summary(mod5)
varb2 <- vcov(mod5)[2,2]
varb2
sighat2 <- smod5$sigma^2
sighat2
varf <- sighat2+sighat2/N+(xeduc-xedbar)^2*varb2
sef <- sqrt(varf)
lnyhat <- b1+b2*xeduc
lowb <- exp(lnyhat-tcr*sef)
upb <- exp(lnyhat+tcr*sef)
lowb
upb 

# The following shows a  95% confidence band for the log-linear wage model.
# Drawing a confidence band for the log-linear
# *wage* equation
xmin <- min(cps4_small$educ)
xmax <- max(cps4_small$educ)+2
education <- seq(xmin, xmax, 2)
lnyhat <- b1+b2*education
yhat <- exp(lnyhat)
varf <- sighat2+sighat2/N+(education-xedbar)^2*varb2
sef <- sqrt(varf)
lowb <- exp(lnyhat-tcr*sef)
upb <- exp(lnyhat+tcr*sef)
plot(cps4_small$educ, cps4_small$wage, col="grey",
     xlab="education", ylab="wage", ylim=c(0,100))
lines(yhat~education, lty=1, col="black")
lines(lowb~education, lty=2, col="blue")
lines(upb~education, lty=2, col="blue")
legend("topleft", legend=c("yhat", "lowb", "upb"),
       lty=c(1, 2, 2), col=c("black", "blue", "blue"))


# 4.6
data("newbroiler", package="PoEdata")

pdf("newbroiler.pdf") 
plot(newbroiler$p, newbroiler$q);
dev.off()

mod6 <- lm(log(q)~log(p), data=newbroiler)
b1 <- coef(mod6)[[1]]
b2 <- coef(mod6)[[2]]
smod6 <- summary(mod6)
smod6

#tbl <- data.frame(xtable(smod6))
#kable(tbl, caption="The log-log poultry regression equation")


# Drawing the fitted values of the log-log equation
ngrid <- 20 # number of drawing points 
xmin <- min(newbroiler$p)
xmax <- max(newbroiler$p)
step <- (xmax-xmin)/ngrid # grid dimension
xp <- seq(xmin, xmax, step)

sighat2 <- smod6$sigma^2
sighat2
yhatc <- exp(b1+b2*log(newbroiler$p)+sighat2/2)

xp = 1.5
yn <- exp(b1+b2*log(xp)) # natural predictor
yn 
yc <- exp(b1+b2*log(xp)+sighat2/2) #corrected predictor
yc 

plot(newbroiler$p, newbroiler$q, ylim=c(10,60),
     xlab="price", ylab="quantity")
lines(yc~xp, lty=1, col="black")

# The generalized R-squared:
rgsq <- cor(newbroiler$q, yhatc)^2
rgsq
